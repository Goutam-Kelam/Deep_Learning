{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Bag of Words Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__*The Continuous Bag-of-Words model (CBOW) is frequently used in NLP deep learning. It is a model that tries to predict words given the context of a few words before and a few words after the target word.\n",
    "The CBOW model is as follows. Given a target word $w_i$ and an $N$ context window on each side,$w_{i-1}, \\dots, w_{i-N}$ and $w_{i+1}, \\dots, w_{i+N}$, referring to all context words collectively as $C$, CBOW tries to minimize\n",
    " $  -\\log p(w_i | C) = -\\log \\text{Softmax}(A(\\sum_{w \\in C} q_w) + b)$where $q_w$ is the embedding for word $w$.*__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__*This is a solution for the exercise problem of word embedding tutorial https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
    "<br>In this problem given two context word to the left and two context word to the right, we have to predict the middle target word.*__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = \"Once Narada met Sanath kumara and asked for enlightenment. \\\n",
    "Sanath kumara asked Narada about the special power which Narada had acquired because of his learning.\\\n",
    "To this Narada replied that he knows all that is contained in the four Vedas and the six Shastras.\\\n",
    "Sanath kumara smiled at this reply and said that while it is a matter of great satisfaction that\\\n",
    "Narada had learnt the Vedas and the Shastras but he would like to ask whether he had learnt anything\\\n",
    "of the self and whether he had understood himself. Sanath kumara then told Narada that so long as one\\\n",
    "does not understand one’s self, the knowledge of all the Shastras, all the Vedas, of the Gita and the \\\n",
    "Upanishads becomes quite useless. Your knowledge will become useful only when you are able to realise the\\\n",
    "nature of the self. What is important is the Adwaitha darshana. You should be able to realise and understand\\\n",
    "the non-dual aspect that is pervading the entire universe. Today in the world, without making an effort to \\\n",
    "understand one’s own self, people are imagining that they are achieving many great things with the help of \\\n",
    "modern science, and in the process they are putting their feet into many difficult situations. By saying that \\\n",
    "they are able to travel far into the sky, see the stars, go to the moon and set up camps there, they are only \\\n",
    "building castles in the air. They may partially succeed in doing such things, but if in the process they do not \\\n",
    "understand the Self and if they do not have peace of mind for themselves, they are very foolish indeed. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 300\n",
    "EPOCH = 40\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating Model\n",
    "class CBOW(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocabulary_size,embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocabulary_size,embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim,512)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(512,vocabulary_size)\n",
    "        self.act2 = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self,input_word_index):\n",
    "        embeddings = sum(self.embedding(input_word_index)) # summing up the embedding of each context word\n",
    "        flatten = embeddings.view(1,-1) \n",
    "        result = self.linear1(flatten)\n",
    "        result = self.act1(result)\n",
    "        result = self.linear2(result)\n",
    "        result = self.act2(result)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(data,vocabulary_size,word2index):\n",
    "    \n",
    "    model = CBOW(vocabulary_size,EMBEDDING_DIM).to(device)\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(),lr = 0.001)\n",
    "    \n",
    "    for epoch in range(EPOCH):\n",
    "        total_loss = 0\n",
    "        for context, target in data:\n",
    "            #converting words to their respective index\n",
    "            context_tensor = torch.cuda.LongTensor([word2index[word] for word in context])\n",
    "            target_tensor = torch.cuda.LongTensor([word2index[target]])\n",
    "            \n",
    "            model.zero_grad()\n",
    "            \n",
    "            log_probability = model(context_tensor)\n",
    "            loss = loss_function(log_probability,target_tensor)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss+=loss.data\n",
    "            \n",
    "        avg_loss = float(total_loss / len(data))\n",
    "        print(\"Epoch: {}/{} \\t Avg_Loss {:.4f}\".format(epoch+1, EPOCH, avg_loss))\n",
    "            \n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda35/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/40 \t Avg_Loss 5.02\n",
      "Epoch: 2/40 \t Avg_Loss 4.37\n",
      "Epoch: 3/40 \t Avg_Loss 3.90\n",
      "Epoch: 4/40 \t Avg_Loss 3.47\n",
      "Epoch: 5/40 \t Avg_Loss 3.08\n",
      "Epoch: 6/40 \t Avg_Loss 2.72\n",
      "Epoch: 7/40 \t Avg_Loss 2.37\n",
      "Epoch: 8/40 \t Avg_Loss 2.05\n",
      "Epoch: 9/40 \t Avg_Loss 1.76\n",
      "Epoch: 10/40 \t Avg_Loss 1.49\n",
      "Epoch: 11/40 \t Avg_Loss 1.25\n",
      "Epoch: 12/40 \t Avg_Loss 1.04\n",
      "Epoch: 13/40 \t Avg_Loss 0.86\n",
      "Epoch: 14/40 \t Avg_Loss 0.71\n",
      "Epoch: 15/40 \t Avg_Loss 0.59\n",
      "Epoch: 16/40 \t Avg_Loss 0.49\n",
      "Epoch: 17/40 \t Avg_Loss 0.42\n",
      "Epoch: 18/40 \t Avg_Loss 0.36\n",
      "Epoch: 19/40 \t Avg_Loss 0.31\n",
      "Epoch: 20/40 \t Avg_Loss 0.28\n",
      "Epoch: 21/40 \t Avg_Loss 0.25\n",
      "Epoch: 22/40 \t Avg_Loss 0.22\n",
      "Epoch: 23/40 \t Avg_Loss 0.20\n",
      "Epoch: 24/40 \t Avg_Loss 0.18\n",
      "Epoch: 25/40 \t Avg_Loss 0.17\n",
      "Epoch: 26/40 \t Avg_Loss 0.16\n",
      "Epoch: 27/40 \t Avg_Loss 0.15\n",
      "Epoch: 28/40 \t Avg_Loss 0.14\n",
      "Epoch: 29/40 \t Avg_Loss 0.13\n",
      "Epoch: 30/40 \t Avg_Loss 0.12\n",
      "Epoch: 31/40 \t Avg_Loss 0.11\n",
      "Epoch: 32/40 \t Avg_Loss 0.11\n",
      "Epoch: 33/40 \t Avg_Loss 0.10\n",
      "Epoch: 34/40 \t Avg_Loss 0.10\n",
      "Epoch: 35/40 \t Avg_Loss 0.09\n",
      "Epoch: 36/40 \t Avg_Loss 0.09\n",
      "Epoch: 37/40 \t Avg_Loss 0.09\n",
      "Epoch: 38/40 \t Avg_Loss 0.08\n",
      "Epoch: 39/40 \t Avg_Loss 0.08\n",
      "Epoch: 40/40 \t Avg_Loss 0.08\n"
     ]
    }
   ],
   "source": [
    "    Token = corpus.split()\n",
    "    vocabulary = set(Token) #Removed Duplicates\n",
    "    vocabulary_size = len(vocabulary)\n",
    "\n",
    "    #creating word to index dictionary\n",
    "    word2index = {word:i for i,word in enumerate(vocabulary)}\n",
    "    index2word = {word:i for word,i in enumerate(vocabulary)}\n",
    "\n",
    "    #storing context and target pair\n",
    "    data = []\n",
    "    for i in range(CONTEXT_SIZE,len(Token)-CONTEXT_SIZE):\n",
    "        context = [Token[i-2],Token[i-1],Token[i+1],Token[i+2]]\n",
    "        target = Token[i]\n",
    "        data.append((context,target))\n",
    "\n",
    "    # Training the model\n",
    "    cbow = train(data,vocabulary_size,word2index)\n",
    "    \n",
    "    # Saving the model\n",
    "    torch.save(cbow.state_dict(),\"./model_save.pth\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(cbow, context_tensor, index2word):    \n",
    "    target = cbow(context_tensor).cpu().data.numpy() \n",
    "    target_index = np.argmax(target) \n",
    "    prediction = index2word[target_index]\n",
    "    print(\"\\nPredicted: {}\\n\\n\".format(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " please enter continuous indices, two to the left and right of the word to be predicted respectively \n",
      "\n",
      "\n",
      " Input seperated by whitespace\n",
      "\n",
      "4 5 7 8\n",
      "\n",
      "Context: ['kumara', 'and', 'for', 'enlightenment.']\n",
      "\n",
      "Predicted: asked\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda35/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# Testing the Model\n",
    "\n",
    "# Loading the saved model\n",
    "cbow = CBOW(vocabulary_size,EMBEDDING_DIM)\n",
    "cbow.load_state_dict(torch.load(\"./model_save.pth\"))\n",
    "cbow.to(device)\n",
    "\n",
    "print(\"\\n please enter continuous indices, two to the left and right of the word to be predicted respectively \\n\")\n",
    "print(\"\\n Input seperated by whitespace\\n\")\n",
    "\n",
    "indices = list(map(int,input().split()))\n",
    "\n",
    "context = [Token[indices[0]], Token[indices[1]], Token[indices[2]], Token[indices[3]]]\n",
    "\n",
    "print(\"\\nContext: {}\".format(context))\n",
    "\n",
    "context_tensor = torch.LongTensor([word2index[word] for word in context]).to(device)   \n",
    "\n",
    "test(cbow, context_tensor, index2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda35/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 99.6183\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating Train Accuracy\n",
    "\n",
    "score = 0\n",
    "for context, target in data:\n",
    "    #converting words to their respective index\n",
    "    context_tensor = torch.cuda.LongTensor([word2index[word] for word in context])\n",
    "    target_tensor = torch.cuda.LongTensor([word2index[target]])\n",
    "    target_pred = cbow(context_tensor).cpu().data.numpy() \n",
    "    target_index = np.argmax(target_pred) \n",
    "    prediction = index2word[target_index]\n",
    "    if(prediction == target):\n",
    "        score+=1\n",
    "\n",
    "print(\"Train Accuracy: {:.4f}\\n\\n\".format((score*100)/len(data)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [anaconda3]",
   "language": "python",
   "name": "Python [anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
